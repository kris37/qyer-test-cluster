<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<?xml-stylesheet type="text/xsl" href="configuration.xsl"?>

<configuration>
 	<property>
    	<name>hive.metastore.warehouse.dir</name>
    	<value>/hive/warehouse</value>
    	<description>location of default database for the warehouse</description>
  	</property>
 <!--thrift metastore，hive.metastore.local 属性已经弃用,如果uris 为 空则为local模式 直接连数据库，否则读取server-->
  	<property>
    	<name>hive.metastore.uris</name>
    	<value>thrift://10.1.10.33:9083</value>
    	<description>Thrift URI for the remote metastore. Used by metastore client to connect to remote metastore.</description>
  	</property>
 	<property>
		<name>hive.exec.mode.local.auto</name>
		<value>false</value>
	</property>

  <!--hive compress settings-->
    <!--map out compress snappy-->

    <property>
      <name>hive.exec.compress.intermediate</name>
      <value>true</value>
    </property>

    <property>
      <name>mapreduce.map.output.compress.codec</name>
      <value>org.apache.hadoop.io.compress.SnappyCodec</value>
    </property>
    <!--job out compress lzop-->
    <property>
      <name>hive.exec.compress.output</name>
      <value>true</value>
    </property>
    <property>
      <name>mapreduce.output.fileoutputformat.compress.codec</name>
      <value>com.hadoop.compression.lzo.LzopCodec</value>
    </property>
	<!--jdbc settings-->
	<property>
  		<name>javax.jdo.option.ConnectionURL</name>
  		<value>jdbc:mysql://10.1.1.193:3307/hive?createDatabaseIfNotExist=true</value>
  		<description>JDBC connect string for a JDBC metastore</description>
	</property>
	<property>
 		 <name>javax.jdo.option.ConnectionDriverName</name>
 		 <value>com.mysql.jdbc.Driver</value>
 		 <description>Driver class name for a JDBC metastore</description>
	</property>
	<property>
  		<name>javax.jdo.option.ConnectionUserName</name>
  		<value>panqiang</value>
  		<description>username to use against metastore database</description>
	</property>
	<property>
  		<name>javax.jdo.option.ConnectionPassword</name>
 		 <value>fs1n75z6</value>
  		<description>password to use against metastore database</description>
	</property>
	<!--merge final reslut files,defalut is false this set true-->
  	<property>
    	<name>hive.merge.mapredfiles</name>
    	<value>true</value>
    	<description>Merge small files at the end of a map-reduce job</description>
  	</property>
	<!--defult is 1009 set min cluster is 200-->
  	<property>
    <name>hive.exec.reducers.max</name>
    <value>200</value>
    <description>
      max number of reducers will be used. If the one specified in the configuration parameter mapred.reduce.tasks is
      negative, Hive will use this one as the max number of reducers when automatically determine number of reducers.
    </description>
  </property>
  <!--exec model-->
  <property>
    <name>hive.mapred.mode</name>
    <value>nonstrict</value>
    <description>
      The mode in which the Hive operations are being performed. 
      In strict mode, some risky queries are not allowed to run. They include:
        Cartesian Product.
        No partition being picked up for a query.
        Comparing bigints and strings.
        Comparing bigints and doubles.
        Orderby without limit.
    </description>
  </property>
  <!--defalut exec queue-->
   <property>
     <name>mapred.job.queue.name</name>
     <value>default</value>
   </property>
   <!--map max input size 256M-->
   <property>
     <name>mapred.max.split.size</name>
     <value>256000000</value>
   </property>
   <!--map min split! input size defalut is 1 means not split but there become mergecombine 100M-->
   <property>
     <name>mapred.min.split.size.per.node</name>
     <value>100000000</value>
   </property>
    <property>
     <name>mapred.min.split.size.per.rack</name>
     <value>100000000</value>
   </property>
   <!--combine input format default-->
   <property>
    <name>hive.input.format</name>
    <value>org.apache.hadoop.hive.ql.io.CombineHiveInputFormat</value>
    <description>The default input format. Set this to HiveInputFormat if you encounter problems with CombineHiveInputFormat.</description>
  </property>
  <!--hive server2-->
   <property>
    <name>hive.server2.thrift.http.port</name>
    <value>10001</value>
    <description>Port number of HiveServer2 Thrift interface when hive.server2.transport.mode is 'http'.</description>
  </property>
  <property>
    <name>hive.server2.thrift.bind.host</name>
    <value/>
    <description>Bind host on which to run the HiveServer2 Thrift service.</description>
  </property>
  <property>
    <name>hive.server2.thrift.port</name>
    <value>10000</value>
    <description>Port number of HiveServer2 Thrift interface when hive.server2.transport.mode is 'binary'.</description>
  </property>
<!--if flase all through server2 submit jobs user is root,else is self users-->
 <property>
    <name>hive.server2.enable.doAs</name>
    <value>true</value>
    <description>
      Setting this property to true will have HiveServer2 execute
      Hive operations as the user making the calls to it.
    </description>
  </property>

</configuration>
